import glob
import re
import pandas as pd
from src.utils.util import clean_groups
from src.python_objects.base import Base
from collections import defaultdict
import os
import subprocess


class MatlabProgramRunner(Base):
    def __init__(self, conversion_years):
        super().__init__(conversion_years)
        self.conversion_years = conversion_years

    def write_matlab_params(self):
        # Get the list of files from the correct directory
        matrices_dir = self.data_path / "matrices"
        files = matrices_dir.glob("*.csv")

        # Extract the max groups
        result = self.extract_max_groups(files)

        # Prepare the output strings
        start_years = []
        end_years = []
        max_groups = []

        for (start, end), max_group in sorted(result.items()):
            start_years.append(str(start))
            end_years.append(str(end))
            max_groups.append(
                str(max_group)
            )  # We'll convert these back to integers in the output

        # Create the output directory if it doesn't exist
        output_dir = self.data_path / "temp"
        os.makedirs(output_dir, exist_ok=True)

        # Write to the file
        output_file = output_dir / "matlab_script_params.txt"
        with open(output_file, "w") as f:
            f.write("# Generated by generator_params.py\n\n")
            # Write each parameter on a new line
            f.write('START_YEARS="' + " ".join(start_years) + '"\n')
            f.write('END_YEARS="' + " ".join(end_years) + '"\n')
            f.write('MAX_GROUPS="' + " ".join(max_groups) + '"\n')

        print(f"Parameters written to {output_file}")

    def extract_max_groups(self, filenames):
        # Dictionary to store max group number for each year pair
        max_groups = defaultdict(int)

        # Regex pattern to extract years and group numbers
        pattern = r"start\.(\d+)\.end\.(\d+)\.group\.(\d+)\.csv"

        for file in filenames:
            match = re.search(pattern, file.name)
            if match:
                start_year = int(match.group(1))
                end_year = int(match.group(2))
                group_num = int(match.group(3))

                # Update max group if larger
                year_pair = (start_year, end_year)
                max_groups[year_pair] = max(max_groups[year_pair], group_num)

        return max_groups

    def run_matlab_optimization(self):
        script_dir = self.root_dir / "src" / "scripts"
        bash_script = script_dir / "run_matlab_optimization.sh"

        os.chmod(bash_script, 0o755)

        try:
            result = subprocess.run(
                ["bash", bash_script], check=True, capture_output=True, text=True
            )

            self.logger.info("MATLAB optimization completed successfully")
            self.logger.info("Output:", result.stdout)
            if result.stderr:
                self.logger.error("Errors:", result.stderr)

        except subprocess.CalledProcessError as e:
            self.logger.error(f"Error running MATLAB optimization: {e}")
            self.logger.error("Error output:", e.stderr)
        except Exception as e:
            self.logger.error(f"Unexpected error: {e}")


class GroupWeights(Base):
    def __init__(self, conversion_years):
        super().__init__(conversion_years)
        self.conversion_years = conversion_years

    def run(self):
        for conversion_pair in self.conversion_years:
            source_class = conversion_pair["source_class"]
            start_year = conversion_pair["source_year"]
            target_class = conversion_pair["target_class"]
            end_year = conversion_pair["target_year"]

            print(f"source class {source_class} and target class {target_class}")
            combined_result = pd.DataFrame()
            weights_dir = self.data_path / "conversion_weights"
            results = weights_dir.glob(
                f"conversion.weights.start.{start_year}.end.{end_year}.group.*.csv"
            )
            if not results:
                raise ValueError("Missing conversion weights data, need to run step 4")
            for file in results:
                match = re.search(r"group\.(\d+)\.csv$", str(file))
                if not match:
                    continue

                gid = match.group(1)

                # try:
                matrices_dir = self.data_path / "matrices"
                conversion_group = pd.read_csv(
                    matrices_dir
                    / f"conversion.matrix.start.{start_year}.end.{end_year}.group.{gid}.csv",
                    dtype={"code.source": str},
                )

                # Load weights and conversion matrix
                weights = pd.read_csv(file, header=None)

                if source_class.startswith("H"):
                    detailed_product_level = 6
                else:
                    detailed_product_level = 4
                # Standardize source product codes
                conversion_group["code.source"] = conversion_group["code.source"].apply(
                    lambda x: (
                        x.zfill(detailed_product_level)
                        if len(x) < detailed_product_level and x != "TOTAL"
                        else x
                    )
                )

                conversion_group = conversion_group.set_index("code.source")
                weight_df = pd.DataFrame(
                    weights.values,
                    index=conversion_group.index,
                    columns=conversion_group.columns,
                )

                # Convert to long format
                weight_long = (
                    weight_df.reset_index()
                    .melt(
                        id_vars="code.source",
                        var_name="code.target",
                        value_name="weight",
                    )
                    .astype({"code.source": str, "code.target": str, "weight": float})
                )

                weight_long["group_id"] = gid
                combined_result = pd.concat([combined_result, weight_long])
                # except:
                #     print("failed")
            groups_dir = self.data_path / "concordance_groups"
            groups = pd.read_csv(
                groups_dir / f"from_{source_class}_to_{target_class}.csv",
                dtype={source_class: str, target_class: str},
            )
            # add back products that did not require optimization and thus never assigned a group id
            non_grouped_products = groups[groups["group.id"].isna()]
            non_grouped_products = clean_groups(
                non_grouped_products, source_class, target_class
            )

            non_grouped_products = non_grouped_products[
                ["group.id", "code.source", "code.target"]
            ]
            non_grouped_products["weight"] = 1
            non_grouped_products = non_grouped_products.rename(
                columns={"group.id": "group_id"}
            )
            combined_result = pd.concat([combined_result, non_grouped_products])

            combined_result[["code.target", "code.source"]] = combined_result[
                ["code.target", "code.source"]
            ].astype(str)
            combined_result = combined_result.rename(
                columns={"code.target": target_class, "code.source": source_class}
            )

            print(f"saving {source_class}:{target_class}.csv")
            combined_result = combined_result[combined_result.weight != 0]
            output_dir = self.data_path / "output" / "grouped_weights"
            os.makedirs(output_dir, exist_ok=True)
            combined_result.to_csv(
                output_dir / f"grouped_{source_class}_{target_class}.csv",
                index=False,
            )
