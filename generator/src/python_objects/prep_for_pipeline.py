import glob
import re
import pandas as pd
from src.utils.util import clean_groups
from collections import defaultdict
import os
import subprocess

class MatlabProgramRunner:
    def __init__(self, conversion_years):
        self.conversion_years = conversion_years

    def write_matlab_params(self):
        # Get the list of files from the correct directory
        matrices_dir = "/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/data/matrices"
        files = glob.glob(os.path.join(matrices_dir, "*.csv"))

        # Extract the max groups
        result = self.extract_max_groups(files)

        # Prepare the output strings
        start_years = []
        end_years = []
        max_groups = []

        for (start, end), max_group in sorted(result.items()):
            start_years.append(str(start))
            end_years.append(str(end))
            max_groups.append(str(max_group))  # We'll convert these back to integers in the output

        # Create the output directory if it doesn't exist
        output_dir = "/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/data/temp"
        os.makedirs(output_dir, exist_ok=True)

        # Write to the file
        output_file = os.path.join(output_dir, "matlab_script_params.txt")
        with open(output_file, 'w') as f:
            f.write("# Generated by generator_params.py\n\n")
            # Write each parameter on a new line
            f.write("START_YEARS=\"" + " ".join(start_years) + "\"\n")
            f.write("END_YEARS=\"" + " ".join(end_years) + "\"\n")
            f.write("MAX_GROUPS=\"" + " ".join(max_groups) + "\"\n")

        print(f"Parameters written to {output_file}")


    def extract_max_groups(self, filenames):
        # Dictionary to store max group number for each year pair
        max_groups = defaultdict(int)
        
        # Regex pattern to extract years and group numbers
        pattern = r'start\.(\d+)\.end\.(\d+)\.group\.(\d+)\.csv'
        
        for filename in filenames:
            match = re.search(pattern, filename)
            if match:
                start_year = int(match.group(1))
                end_year = int(match.group(2))
                group_num = int(match.group(3))
                
                # Update max group if larger
                year_pair = (start_year, end_year)
                max_groups[year_pair] = max(max_groups[year_pair], group_num)
        
        return max_groups
    

    def run_matlab_optimization(self):
        script_dir = "/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/src/scripts/"
        bash_script = os.path.join(script_dir, "run_matlab_optimization.sh")
        
        # Make sure the script is executable
        os.chmod(bash_script, 0o755)
        
        # Run the bash script using subprocess
        try:
            result = subprocess.run(['bash', bash_script], 
                                check=True,
                                capture_output=True,
                                text=True)
            
            print("MATLAB optimization completed successfully")
            print("Output:", result.stdout)
            if result.stderr:
                print("Errors:", result.stderr)
                
        except subprocess.CalledProcessError as e:
            print(f"Error running MATLAB optimization: {e}")
            print("Error output:", e.stderr)
        except Exception as e:
            print(f"Unexpected error: {e}")


# converted_files_dtypes = {
#         "reporterCode":str,
#         "flowCode":str,
#         "partnerCode":str,
#         "cmdCode": str,
#         "qty":float,
#         "CIFValue":float,
#         "FOBValue":float,
#         "primaryValue":float,
# }

# RELEASE_YEARS = {"S1" : 1962, "S2": 1976, "S3": 1988, #"S4": 2007,
#                  "H0": 1988, "H1": 1996, "H2": 2002, "H3": 2007, "H4": 2012, "H5": 2017, "H6": 2022}
 
# conversion_links = ["H6", "H5", "H4", "H3", "H2", "H1", "H0", "S3", "S2", "S1"]

# atlas_classes = ["H0"] #"H0", 

# for direction in ["backward", "forward"]:
#     if direction == "backward":
#         conversion_links.reverse()
#         final_class = "H0"
#     else:
#         final_class = "H6"
    
#     print(f"starting {direction} in this order {conversion_links}")

#     for i, source_class in enumerate(conversion_links):
#         if source_class == final_class:
#             # set up to move raw parquet as reported directly to converted 
#             continue
#         next_class = conversion_links[i + 1]
        
#         if source_class.startswith("H"):
#             detailed_product_level = 6
#         else:
#             detailed_product_level = 4
#         if next_class.startswith("H"):
#             detailed_product_level = 6
#         else:
#             detailed_product_level = 4

#         print(f"next class {next_class}")
#         if direction == "backward":
#             source_year = RELEASE_YEARS[source_class]
#             target_year = source_year - 1
#         else: # forward
#             target_year = RELEASE_YEARS[next_class]
#             source_year = target_year - 1
            
#         if source_class in ["H0", "S3"] and target_class in ["H0", "S3"]:
#             if source_class == "H0":
#                 source_year = 1992
#                 target_year = 1988
#             else:
#                 source_year = 1988
#                 target_year = 1992

#         for as_reported_year in range(start_year, end_year):
#             print(f"converting {as_reported_year} from {source_class} to {next_class}.")
#             logging.info(f"converting {as_reported_year} from {source_class} to {next_class}.")
#             files_source = glob.glob(f"/n/hausmann_lab/lab/atlas/data/as_reported/raw_parquet/{source_class}/{as_reported_year}/*.parquet")
#             # intermediate_files = glob.glob(f"/n/hausmann_lab/lab/atlas/data/as_reported/intermediate_converted/{source_class}/{as_reported_year}/*.parquet")
#             # files_source = files_source + intermediate_files
#             for file in files_source:
#                 file = file.split('/')[-1]
#                 result = pd.DataFrame()
#                 try:
#                     source = pd.read_parquet(f"/n/hausmann_lab/lab/atlas/data/as_reported/raw_parquet/{source_class}/{as_reported_year}/{file}")
#                     import pdb
#                     pdb.set_trace()
#                 except:
#                     continue
#                 source['cmdCode'] = source.cmdCode.astype(str)
#                 reporter_code = ComtradeFile(file).reporter_code
#                 logging.info(f"reporter: {reporter_code}, file being converted: {file}")
#                 # generate group_dfs
#                 for group_file in glob.glob(f"/n/hausmann_lab/lab/atlas/bustos_yildirim/paper/product_conversion/DataAtlas/conversion.matrix.start.{source_year}.end.{target_year}.group.*.csv"):
#                     match = re.search(r'group\.(\d+)\.csv$', group_file)
#                     gid = match.group(1)
#                     # group_df = pd.read_csv(group_file, index_col=None)
#                     weights = pd.read_csv(f"Results/conversion.weights.start.{source_year}.end.{target_year}.group.{gid}.csv", header=None)
#                     conversion = pd.read_csv(f"DataAtlas/conversion.matrix.start.{source_year}.end.{target_year}.group.{gid}.csv")
#                     conversion['code.source'] = conversion['code.source'].astype(str)
#                     conversion.loc[conversion['code.source'].str.len().isin([5, 4, 3]), "code.source"] = conversion['code.source'].str.zfill(6)

#                     conversion = conversion.set_index('code.source')
#                     # filter for products in group
#                     source_products = conversion.index.tolist()
#                     source_group = source[source.cmdCode.isin([str(source) for source in source_products])]
#                     df = run_conversion(weights, conversion, source_group, detailed_product_level)
#                     import pdb; pdb.set_trace()

#                     df['group_id'] = gid
#                     result = pd.concat([result, df], axis=0)
#                 # handle 1:1, n:1 matches
#                 groups = pd.read_csv(f"data_groups/from_{source_class}_to_{next_class}.csv")
#                 matched_groups, _ = clean_groups(groups, source_class, next_class)
#                 matched_products = matched_groups['code.source'].unique().tolist()
#                 matched = source[source['cmdCode'].isin(matched_products)]
                    
#                 for col in ['code.source','code.target']:
#                     matched_groups[col] = matched_groups[col].astype(str)
#                     matched_groups[col] = matched_groups[col].apply(
#                         lambda x: x.zfill(detailed_product_level) if len(x) < detailed_product_level and x != "TOTAL" else x
#                         )
                    
#                 matched = matched.merge(matched_groups[['code.source','code.target']], left_on='cmdCode', right_on='code.source', how='left').drop(columns=['code.source','cmdCode'])

#                 if not matched.empty:
#                     matched = matched.rename(columns={"code.target":"cmdCode"})
#                     result = pd.concat([result, matched])

#                 import pdb; pdb.set_trace()
#                 result = add_product_levels(result)
#                 result['isAggregate'] = 1
#                 result = result.drop(columns='level')
#                 result = result.astype(converted_files_dtypes)
#                 f = ComtradeFile(file)
#                 f.swap_classification(next_class)
#                 path = Path(f"/n/hausmann_lab/lab/atlas/data/as_reported/intermediate_converted/{next_class}/{as_reported_year}")
#                 path.mkdir(parents=True, exist_ok=True)
                
#                 for col in result.select_dtypes(include=['string', 'object', 'category']).columns:
#                     result[col] = result[col].astype(str)

#                 result.to_parquet(f"/n/hausmann_lab/lab/atlas/data/as_reported/intermediate_converted/{next_class}/{as_reported_year}/{f.name}", index=False)
#                 logging.info(f"saving to intermediate converted file {f.name}")
#                 if next_class in atlas_classes:
#                     path = Path(f"/n/hausmann_lab/lab/atlas/data/as_reported/converted/{next_class}/{as_reported_year}")
#                     path.mkdir(parents=True, exist_ok=True)
#                     result.to_parquet(f"/n/hausmann_lab/lab/atlas/data/as_reported/converted/{next_class}/{as_reported_year}/{f.name}", index=False)
#                     logging.info(f"saving to final converted file {f.name}")



class GroupWeights:
    def __init__(self):
        pass

    def run(self):
        for source_class, start_year, target_class, end_year in self.conversion_years:
            combined_result = pd.DataFrame()
            results = glob.glob(f"/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/data/conversion_weights/conversion.weights.start.{start_year}.end.{end_year}.group.*.csv")
            for file in results:
                match = re.search(r'group\.(\d+)\.csv$', str(file))
                if not match:
                    continue

                gid = match.group(1)

                # try:
                conversion_group = pd.read_csv(f"/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/data/matrices/conversion.matrix.start.{start_year}.end.{end_year}.group.{gid}.csv",
                                            dtype={'code.source': str})

                # Load weights and conversion matrix
                weights = pd.read_csv(file, header=None)

                if source_class.startswith("H"):
                    detailed_product_level = 6
                else:
                    detailed_product_level = 4
                # Standardize source product codes
                conversion_group['code.source'] = conversion_group['code.source'].apply(
                    lambda x: x.zfill(detailed_product_level) if len(x) < detailed_product_level and x != "TOTAL" else x
                )

                conversion_group = conversion_group.set_index('code.source')            
                weight_df = pd.DataFrame(weights.values, index=conversion_group.index,columns=conversion_group.columns)

                # Convert to long format
                weight_long = weight_df.reset_index().melt(
                    id_vars='code.source',var_name='code.target',value_name='weight'
                ).astype({'code.source': str, 'code.target':str, 'weight': float})

                weight_long['group_id'] = gid
                combined_result = pd.concat([combined_result, weight_long])
                # except:
                #     print("failed")
                    
            groups = pd.read_csv(f"/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/data/concordance_groups/from_{source_class}_to_{target_class}.csv", 
                                dtype={source_class: str, target_class: str})
            matched, cleaned_group = clean_groups(groups, source_class, target_class)

            matched = matched[['group.id','code.source','code.target']]
            matched['weight'] = 1
            matched = matched.rename(columns={"group.id":"group_id"})
            combined_result = pd.concat([combined_result, matched])
            
            combined_result[['code.target','code.source']] = combined_result[['code.target','code.source']].astype(str)
            combined_result = combined_result.rename(columns={"code.target":target_class,"code.source":source_class})
                    
            print(f"saving {source_class}:{target_class}.csv")
            combined_result.to_csv(f"/n/hausmann_lab/lab/atlas/bustos_yildirim/weights_generator/generator/data/output/grouped_weights/grouped_{source_class}:{target_class}.csv", index=False)       
